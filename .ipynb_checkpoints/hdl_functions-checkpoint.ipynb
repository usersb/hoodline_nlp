{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HTML_TO_TEXT(html):\n",
    "    soup = BeautifulSoup(html,\"lxml\")\n",
    "    all_text = soup.findAll(text = True)\n",
    "    text = ''.join(all_text)\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(summary):\n",
    "    \n",
    "    tokens = [word for sent in nltk.sent_tokenize(summary) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]',token):\n",
    "            filtered_tokens.append(token)\n",
    "    \n",
    "    #stem each of the filtered tokens\n",
    "    stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "      \n",
    "    return(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(summary):\n",
    "    \n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(summary) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]',token):\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "    return(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_link(summary):\n",
    "    return(re.findall(r'(http(s)?://[^\\s]+)',summary))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    pdata = data\n",
    "    pdata['summary'],pdata['links'],pdata['stemmed'],pdata['tokens']='','','',''\n",
    "    \n",
    "    \n",
    "    pdata['summary'] = pdata['content'].apply(HTML_TO_TEXT) #converting html to text\n",
    "    pdata['summary'] = pdata['summary'].str.lower()\n",
    "    pdata['links'] = pdata['content'].apply(retrieve_link)    \n",
    "    pdata['stemmed'] = pdata['summary'].apply(tokenize_and_stem)\n",
    "    pdata['tokens'] = pdata['summary'].apply(tokenize)\n",
    "    \n",
    "    return(pdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vocab_frame(data):\n",
    "    \n",
    "    all_words_stemmed = []\n",
    "    all_words_tokenized = []\n",
    "    \n",
    "   \n",
    "    for i in data['summary']:\n",
    "        words_stemmed = tokenize_and_stem(i)\n",
    "        all_words_stemmed.extend(words_stemmed)\n",
    "        \n",
    "        words_tokenized = tokenize(i)\n",
    "        all_words_tokenized.extend(words_tokenized)\n",
    "        \n",
    "    return(pd.DataFrame({'words': all_words_tokenized}, index=all_words_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate tfidf_matrix\n",
    "\n",
    "def generate_tfidf(data):\n",
    "    \n",
    "    tfidfvectorizer = TfidfVectorizer(max_df=0.85, max_features=200000,min_df=0.20,stop_words='english',use_idf=True,\n",
    "                                      tokenizer = tokenize_and_stem, ngram_range = (1,3))\n",
    "    \n",
    "    tfidf_matrix = tfidfvectorizer.fit_transform(data)\n",
    "    terms = tfidfvectorizer.get_feature_names()\n",
    "    distance = 1 - cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    return(tfidf_matrix,terms,distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_num_clusters_k(data):\n",
    "    ks = range(1,20)\n",
    "    inertias = []\n",
    "    \n",
    "    for k in ks:\n",
    "        kmeans_model = KMeans(n_clusters=k)\n",
    "        kmeans_model.fit(data)\n",
    "        inertias.append(kmeans_model.inertia_)\n",
    "    \n",
    "    \n",
    "    plt.plot(ks,inertias,'-o')\n",
    "    plt.xlabel('Number of clusters k')\n",
    "    plt.ylabel('inertia')\n",
    "    plt.xticks(ks)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_kmeans(data,num_clusters):\n",
    "    \n",
    "    kmeans_model = KMeans(n_clusters=num_clusters)\n",
    "    kmeans_model.fit(data)\n",
    "    clusters = kmeans_model.labels_\n",
    "    \n",
    "    joblib.dump(kmeans_model,'kmeans_model.pkl') #save kmeans model\n",
    "    return(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_headlines(data,num_clusters,num_headlines):\n",
    "    \n",
    "    for n in range(num_clusters):\n",
    "        print('-----------------------------------------------------------------------------')\n",
    "        counter = 0\n",
    "        for i in range(len(data)):\n",
    "            if data['cluster'][i] == n and counter < num_headlines:\n",
    "                print(data['headline'][i])\n",
    "                print(\"\\n\")\n",
    "                counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_nmf(tfidf_matrix,num_components):\n",
    "    \n",
    "    nmf_model = NMF(n_components=num_components)\n",
    "    nmf_model.fit(tfidf_matrix)\n",
    "    \n",
    "    nmf_features = nmf_model.transform(tfidf_matrix)\n",
    "    nmf_components = nmf_model.components_\n",
    "    \n",
    "    return(nmf_features,nmf_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
